{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Dairy Estrus LSTM (Colab-ready)\n- ≤6h **inputs-only linear interpolation**, >6h **segment split**\n- Features: **활동량, 전체 반추 시간(분)**; Label: horizon point **발정 확률 ≥ 25**\n- Split: **cow-wise 80/20**\n- Normalization: **fit on Train → transform Train/Val**"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import os, glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import precision_recall_curve, roc_curve, roc_auc_score, auc, classification_report, confusion_matrix\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"DEVICE:\", DEVICE)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Run this cell first in Colab.\n# It tries, in order:\n# 1) existing `data` variable\n# 2) 'cow_data.csv'\n# 3) './cow/*.csv' or './*.csv'\n# 4) prompt upload UI (Colab) and then parse uploaded CSVs\n\ndef ensure_data():\n    import pandas as pd, numpy as np, os, glob\n    # 1) in-memory\n    if 'data' in globals() and isinstance(data, pd.DataFrame):\n        print(\"Using in-memory `data`:\", data.shape)\n        return data\n\n    # util: read CSV with fallback encodings\n    def _read_csv_any(p):\n        for enc in ['utf-8', 'cp949', 'euc-kr', 'latin1']:\n            try:\n                return pd.read_csv(p, low_memory=False, encoding=enc)\n            except Exception:\n                continue\n        raise RuntimeError(f\"Failed to read {p} with common encodings.\")\n\n    # 2) cow_data.csv\n    if os.path.exists('cow_data.csv'):\n        df = _read_csv_any('cow_data.csv')\n        if 'datetime' not in df.columns:\n            if set(['날짜','시간(시:분)']).issubset(df.columns):\n                df['datetime'] = pd.to_datetime(df['날짜'].astype(str)+' '+df['시간(시:분)'].astype(str), errors='coerce')\n            elif '날짜' in df.columns:\n                df['datetime'] = pd.to_datetime(df['날짜'], errors='coerce')\n        else:\n            df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n        keep = [c for c in ['개체 번호','datetime','활동량','전체 반추 시간(분)','발정 확률'] if c in df.columns]\n        if not {'개체 번호','datetime','활동량','전체 반추 시간(분)'} <= set(keep):\n            print(\"cow_data.csv lacks required columns, trying other sources...\")\n        else:\n            out = df[keep].copy().dropna(subset=['개체 번호','datetime'])\n            out = out.sort_values(['개체 번호','datetime']).reset_index(drop=True)\n            print(\"Loaded cow_data.csv:\", out.shape)\n            return out\n\n    # 3) glob\n    paths = glob.glob('./cow/*.csv') or glob.glob('./*.csv')\n    if len(paths) == 0:\n        # 4) Upload UI (Colab)\n        try:\n            from google.colab import files\n            print(\"No CSVs found. Open the upload dialog and select your csv files.\")\n            uploaded = files.upload()\n            paths = list(uploaded.keys())\n        except Exception as e:\n            raise RuntimeError(\"No data found. Upload CSVs or provide `data` DataFrame.\") from e\n\n    tmp = []\n    for p in paths:\n        try:\n            df = _read_csv_any(p)\n        except Exception as e:\n            print(\"Skip:\", p, e); continue\n\n        # unify time columns\n        if '시간' in df.columns and '시간(시:분)' not in df.columns:\n            df.rename(columns={'시간':'시간(시:분)'}, inplace=True)\n\n        # build datetime if missing\n        if 'datetime' not in df.columns:\n            if set(['날짜','시간(시:분)']).issubset(df.columns):\n                df['datetime'] = pd.to_datetime(df['날짜'].astype(str)+' '+df['시간(시:분)'].astype(str), errors='coerce')\n            elif '날짜' in df.columns:\n                df['datetime'] = pd.to_datetime(df['날짜'], errors='coerce')\n\n        # select columns if present\n        keep = [c for c in ['개체 번호','datetime','활동량','전체 반추 시간(분)','발정 확률'] if c in df.columns]\n        if {'개체 번호','datetime','활동량','전체 반추 시간(분)'} <= set(keep):\n            tmp.append(df[keep].copy())\n\n    if not tmp:\n        raise RuntimeError(\"No usable CSVs. Need columns: 개체 번호, datetime(or 날짜+시간), 활동량, 전체 반추 시간(분).\")\n\n    out = pd.concat(tmp, ignore_index=True).dropna(subset=['개체 번호','datetime'])\n    out['datetime'] = pd.to_datetime(out['datetime'], errors='coerce')\n    out = out.dropna(subset=['datetime']).sort_values(['개체 번호','datetime']).reset_index(drop=True)\n    print(\"Merged from CSVs:\", out.shape)\n    return out\n\ndata = ensure_data()\ndata.head()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "FEATURES = ['활동량','전체 반추 시간(분)']\nTARGET   = '발정 확률'\n\nSEQ_LEN        = 12\nHORIZON_STEPS  = 12\nPOS_THRESH     = 25.0\nprint(\"Params:\", dict(SEQ_LEN=SEQ_LEN, HORIZON_STEPS=HORIZON_STEPS, POS_THRESH=POS_THRESH))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "hours_per_step = (\n    data.groupby('개체 번호')['datetime']\n        .apply(lambda s: s.sort_values().diff())\n        .dropna()\n        .dt.total_seconds().median()\n) / 3600.0\n\nSTEP_MIN = int(round((hours_per_step if not np.isnan(hours_per_step) else 2.0) * 60))\nSHORT_GAP_MAX = 360\nLONG_GAP_CUT  = 360\nprint(f\"Estimated step: ~{hours_per_step:.2f} h/step  → STEP_MIN={STEP_MIN} min\")\n\ndf = data.sort_values(['개체 번호','datetime']).copy()\ndf['gap_min'] = (\n    df.groupby('개체 번호', sort=False)['datetime']\n      .diff().dt.total_seconds().div(60)\n)\ndf['segment_id'] = (\n    df.groupby('개체 번호', sort=False)['gap_min']\n      .transform(lambda s: s.fillna(0).gt(LONG_GAP_CUT).cumsum())\n      .astype('int64')\n)\nprint(\"Segments:\", df.groupby(['개체 번호','segment_id']).size().shape[0])"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def _fill_short_gaps(seg, features, target, step_min, short_gap_max):\n    seg = seg.set_index('datetime').sort_index()\n    full_idx = pd.date_range(seg.index.min(), seg.index.max(), freq=f'{step_min}min')\n    seg = seg.reindex(full_idx)\n\n    limit_steps = int(short_gap_max // step_min)\n    for c in features:\n        seg[c] = pd.to_numeric(seg[c], errors='coerce').interpolate(\n            method='linear', limit=limit_steps, limit_direction='both'\n        )\n    seg[target] = pd.to_numeric(seg[target], errors='coerce')\n    seg['개체 번호'] = seg['개체 번호'].ffill().bfill()\n    seg['segment_id'] = seg['segment_id'].ffill().bfill()\n    return seg.reset_index().rename(columns={'index':'datetime'})\n\nparts = []\nfor (cid, sid), g in df.groupby(['개체 번호','segment_id'], sort=False):\n    parts.append(_fill_short_gaps(g, FEATURES, TARGET, STEP_MIN, SHORT_GAP_MAX))\n\ndata_seg = (pd.concat(parts, ignore_index=True)\n              .sort_values(['개체 번호','datetime'])\n              .reset_index(drop=True))\n\ncols_keep = ['개체 번호','datetime'] + FEATURES + [TARGET,'segment_id']\ndata_seg = data_seg[cols_keep]\nprint(\"Post-interp shape:\", data_seg.shape)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def make_sequences_point_segmented(df, seq_len, horizon_steps, pos_thresh):\n    X, y, cows, times = [], [], [], []\n    skipped_no_label = 0\n\n    for (cid, sid), g in df.groupby(['개체 번호','segment_id']):\n        g = g.sort_values('datetime')\n        if len(g) <= seq_len + horizon_steps - 1:\n            continue\n\n        feats = g[FEATURES].to_numpy(np.float32)\n        prob  = g[TARGET].to_numpy(np.float32)\n        tms   = g['datetime'].to_numpy()\n\n        last_start = len(g) - seq_len - horizon_steps + 1\n        for i in range(last_start):\n            tgt = i + seq_len + horizon_steps - 1\n            if np.isnan(prob[tgt]):\n                skipped_no_label += 1\n                continue\n            X.append(feats[i:i+seq_len, :])\n            y.append(1.0 if prob[tgt] >= pos_thresh else 0.0)\n            cows.append(cid)\n            times.append(tms[i+seq_len-1])\n\n    return (np.asarray(X, np.float32),\n            np.asarray(y, np.float32),\n            np.asarray(cows),\n            np.asarray(times),\n            skipped_no_label)\n\nX, y, cows, seq_end_times, skipped = make_sequences_point_segmented(\n    data_seg, SEQ_LEN, HORIZON_STEPS, POS_THRESH\n)\n\nprint(f\"X: {X.shape} | y: {y.shape} | pos_rate={float(y.mean()):.4f} | skipped(no-label)={skipped:,}\")\nprint(f\"(1 step ≈ {STEP_MIN/60:.1f}h) lookback≈{SEQ_LEN*STEP_MIN/60:.1f}h, horizon≈{HORIZON_STEPS*STEP_MIN/60:.1f}h\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def split_by_cow(cows, train_ratio=0.8, seed=42):\n    rng = np.random.default_rng(seed)\n    uniq = np.unique(cows)\n    rng.shuffle(uniq)\n    n_tr = int(len(uniq) * train_ratio)\n    tr_ids = set(uniq[:n_tr])\n    tr_mask = np.isin(cows, list(tr_ids))\n    va_mask = ~tr_mask\n    return tr_mask, va_mask\n\ntrain_mask, val_mask = split_by_cow(cows, train_ratio=0.8, seed=42)\n\nX_train, y_train = X[train_mask], y[train_mask]\nX_val,   y_val   = X[val_mask],   y[val_mask]\n\n# No cow leakage\ntrain_cows = np.unique(cows[train_mask]); val_cows = np.unique(cows[val_mask])\nprint(\"Train:\", X_train.shape, \" Val:\", X_val.shape, \"| Cow overlap:\", len(set(train_cows) & set(val_cows)))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "scaler = StandardScaler().fit(X_train.reshape(-1, X_train.shape[-1]))\n\ndef apply_scale(X, scaler):\n    N, T, F = X.shape\n    Xf = X.reshape(-1, F)\n    Xf = scaler.transform(Xf)\n    return Xf.reshape(N, T, F)\n\nX_train = apply_scale(X_train, scaler)\nX_val   = apply_scale(X_val,   scaler)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "class SeqDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X)\n        self.y = torch.from_numpy(y).float().unsqueeze(1)\n    def __len__(self): return len(self.X)\n    def __getitem__(self, i): return self.X[i], self.y[i]\n\npin = (DEVICE=='cuda')\ntrain_loader = DataLoader(SeqDS(X_train, y_train), batch_size=32, shuffle=True, drop_last=True, pin_memory=pin, num_workers=2)\nval_loader   = DataLoader(SeqDS(X_val,   y_val),   batch_size=32, shuffle=False, pin_memory=pin, num_workers=2)\n\nclass LSTMClassifier(nn.Module):\n    def __init__(self, input_size, hidden=64, layers=2, dropout=0.2):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size, hidden, num_layers=layers, batch_first=True, dropout=dropout)\n        self.fc   = nn.Linear(hidden, 1)\n    def forward(self, x):\n        out, _ = self.lstm(x)\n        last = out[:, -1, :]\n        return self.fc(last)\n\nmodel = LSTMClassifier(input_size=X.shape[-1]).to(DEVICE)\n\npos = float(y_train.sum()); neg = float(len(y_train) - pos); eps = 1e-6\npos_weight = torch.tensor(neg / max(eps, pos), device=DEVICE, dtype=torch.float32)\ncriterion  = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\noptimizer  = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# Train\nEPOCHS = 15\ntr_hist, va_hist = [], []\n\ndef run_epoch(loader, train=True):\n    model.train() if train else model.eval()\n    total = 0.0\n    with torch.set_grad_enabled(train):\n        for xb, yb in loader:\n            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            if train:\n                optimizer.zero_grad(); loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                optimizer.step()\n            total += loss.item()\n    return total/len(loader)\n\nfor ep in range(1, EPOCHS+1):\n    tr = run_epoch(train_loader, True)\n    va = run_epoch(val_loader,   False)\n    tr_hist.append(tr); va_hist.append(va)\n    print(f\"Epoch {ep:02d} | train={tr:.4f}  val={va:.4f}\")\n\nplt.figure(); plt.plot(tr_hist, label='train'); plt.plot(va_hist, label='val'); plt.title('Loss')\nplt.legend(); plt.grid(True, alpha=0.3); plt.show()\n\n# Eval\nmodel.eval()\nall_logits, all_y = [], []\nwith torch.no_grad():\n    for xb, yb in val_loader:\n        lg = model(xb.to(DEVICE)).cpu().numpy().ravel()\n        all_logits.append(lg); all_y.append(yb.numpy().ravel())\nlogits = np.concatenate(all_logits); y_true = np.concatenate(all_y)\ny_prob = 1/(1+np.exp(-logits))\n\nprec, rec, ths = precision_recall_curve(y_true, y_prob)\nf1s = 2*prec*rec/(prec+rec+1e-9)\nbest_idx = int(np.nanargmax(f1s))\nbest_thr = ths[best_idx] if best_idx < len(ths) else 0.5\npr_auc = auc(rec, prec); from sklearn.metrics import roc_auc_score, roc_curve\nroc_auc = roc_auc_score(y_true, y_prob)\ny_pred = (y_prob >= best_thr).astype(int)\n\nprint(f\"[VAL] PR-AUC={pr_auc:.4f}  ROC-AUC={roc_auc:.4f}  BestF1={f1s[best_idx]:.4f} @thr={best_thr:.3f}\")\nprint(classification_report(y_true, y_pred, target_names=['No Estrus','Estrus'], digits=4))\nprint(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred))\n\n# Curves\nplt.figure(); plt.plot(rec, prec, label=f\"PR (AUC={pr_auc:.3f})\")\nplt.scatter(rec[best_idx], prec[best_idx], label='Best-F1'); plt.xlabel('Recall'); plt.ylabel('Precision')\nplt.grid(True, alpha=0.3); plt.legend(); plt.show()\n\nfpr, tpr, _ = roc_curve(y_true, y_prob)\nplt.figure(); plt.plot([0,1],[0,1],'--'); plt.plot(fpr, tpr, label=f\"ROC (AUC={roc_auc:.3f})\")\nplt.xlabel('FPR'); plt.ylabel('TPR'); plt.grid(True, alpha=0.3); plt.legend(); plt.show()"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}